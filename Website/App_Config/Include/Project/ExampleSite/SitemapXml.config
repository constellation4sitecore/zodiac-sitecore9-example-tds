<?xml version="1.0"?>
<configuration xmlns:patch="http://www.sitecore.net/xmlconfig/">
	<sitecore>
		<!--

		Purpose: This include file shows how to configure a site-specific custom crawler for Sitemap.XML file generation.

		To enable this, rename this file so that it has a ".config" extension and 
		change all the parameters to suit your own scenario

		-->
		
		<constellation>
			<robotsTxt>
				<!--
				SITE RULES

				You can customize the Allows and Disallows that are rendered in the robots.txt by site. Create a siteRules element as 
				shown below where the <siteName/> element is named after the sitecore <site/> element's name attribute value.
				
				NOTE THAT SETTING SITE RULES FOR A SITE WILL COMPLETELY OVERRIDE THE DEFAULT RULES
				
				You can't use both. However, Global rules will be included in every robots.txt response regardless of whether the site
				has unique rules.
				-->
				<siteRules>
					<examplesite>
						<allow>/</allow>
					</examplesite>
				</siteRules>
				<!--
				Note that if you disallow the root of a site: <disallow>/</disallow> then the robots.txt will not include the sitemap.xml reference.
				-->
			</robotsTxt>
			<sitemapXml>
				<crawlers>
					<!--<site name="projectName">
						<crawler type="ProjectName.Crawlers.TreeCrawler, ProjectName" />
					</site>-->
				</crawlers>
			</sitemapXml>
		</constellation>
	</sitecore>
</configuration>
